{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Matemática Para Machine Learning</font>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lab 4 - Reinforcement Learning (A Matemática da Aprendizagem Por Reforço)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### O Que é Reinforcement Learning?\r\n",
    "\r\n",
    "Considere o jogo Breakout. Neste jogo você controla uma pá na parte inferior da tela e tem que disparar a bola de volta para limpar todos os tijolos na metade superior da tela. Cada vez que você bate em um tijolo, ele desaparece e sua pontuação aumenta - você recebe uma recompensa. Para aprender a jogar o Breakout, confira algumas dicas aqui: https://www.youtube.com/watch?v=JRAPnuwnpRs\r\n",
    "\r\n",
    "Suponha que você queira ensinar uma rede neural a jogar este jogo. A entrada em sua rede seria imagens de tela, e a saída seria de três ações: esquerda, direita ou disparar (para iniciar a bola). Teria sentido tratá-lo como um problema de classificação - para cada tela de jogo você deve decidir, se você deve mover para a esquerda, direita ou disparar. Parece simples, mas então você precisa de exemplos de treinamento, e muitos deles. Claro que você poderia gravar sessões de jogos usando jogadores experientes, mas isso não é realmente como aprendemos. Nós apenas precisamos de feedback ocasional que fizemos a coisa certa e podemos descobrir todos os outros nós mesmos.\r\n",
    "\r\n",
    "<img src=\"images/breakout.png\">\r\n",
    "\r\n",
    "Esta é a tarefa que o aprendizado por reforço tenta resolver. O aprendizado por reforço situa-se entre a aprendizagem supervisionada e não supervisionada. No aprendizado supervisionado, usamos um rótulo alvo para cada exemplo de treinamento e na aprendizagem sem supervisão não tem rótulos. No aprendizado por reforço, temos rótulos dispersos e atrasados, as recompensas. Com base apenas nas recompensas, o agente deve aprender a comportar-se no ambiente.\r\n",
    "\r\n",
    "Embora a ideia seja bastante intuitiva, na prática há inúmeros desafios. Por exemplo, quando você bate em um tijolo e ganha uma recompensa no jogo Breakout, muitas vezes não tem nada a ver com as ações que você fez antes de obter a recompensa. Todo o trabalho duro já foi feito, quando você colocou a pá corretamente e disparou a bola de volta. Isso é chamado de problema de atribuição de crédito - ou seja, qual das ações anteriores foi responsável por receber a recompensa e até que ponto.\r\n",
    "\r\n",
    "Depois de ter descoberto uma estratégia para coletar um certo número de recompensas, você deve ficar com ela ou experimentar alguma coisa que possa resultar em recompensas ainda maiores? No jogo Breakout acima, uma estratégia simples é mover-se para a margem esquerda e esperar lá. Quando lançada, a bola tende a voar para a esquerda com mais frequência do que a direita e você conseguirá marcar com facilidade cerca de 10 pontos antes de morrer. Você ficará satisfeito com isso ou quer mais? Isso é chamado de dilema explore-exploit - você deve explorar a estratégia de trabalho conhecida ou explorar outras estratégias possivelmente melhores?\r\n",
    "\r\n",
    "O aprendizado por reforço é um modelo importante de como nós (e todos os animais em geral) aprendemos. Elogios de nossos pais, notas na escola, salário no trabalho - estes são exemplos de recompensas. Problemas de atribuição de crédito e dilemas explore-exploit surgem todos os dias, tanto nos negócios como nos relacionamentos. É por isso que é importante estudar este problema, e os jogos formam uma sandbox maravilhosa para experimentar novas abordagens."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Processo de Decisão de Markov\r\n",
    "\r\n",
    "Agora, a questão é como você formaliza um problema de aprendizado por reforço, para que você possa raciocinar sobre ele? O método mais comum é representá-lo como um Processo de Decisão de Markov.\r\n",
    "\r\n",
    "Suponha que você seja um agente, situado em um ambiente (por exemplo, o jogo Breakout). O ambiente está em certo estado (por exemplo, localização da pá, localização e direção da bola, existência de cada tijolo e assim por diante). O agente pode executar certas ações no ambiente (por exemplo, mover a pá para a esquerda ou para a direita). Essas ações às vezes resultam em uma recompensa (por exemplo, aumento na pontuação). As ações transformam o ambiente e conduzem a um novo estado, onde o agente pode executar outra ação, e assim por diante. As regras sobre como você escolhe essas ações são chamadas de política. O ambiente em geral é estocástico, o que significa que o próximo estado pode ser um pouco aleatório (por exemplo, quando você perde uma bola e lança uma nova, vai para uma direção aleatória).\r\n",
    "\r\n",
    "<img src=\"images/mdp.png\">\r\n",
    "\r\n",
    "O conjunto de estados e ações, juntamente com as regras para a transição de um estado para outro, compõem um Processo de Decisão de Markov. Um episódio deste processo (por exemplo, um jogo) forma uma sequência finita de estados, ações e recompensas:\r\n",
    "\r\n",
    "<img src=\"images/mdp2.png\">\r\n",
    "\r\n",
    "Aqui si representa o estado, ai é a ação e ri + 1 é a recompensa depois de executar a ação. O episódio termina com o estado terminal sn (por exemplo, \"game over\"). Um processo de decisão de Markov baseia-se na suposição de Markov, que a probabilidade de o próximo estado si + 1 depende apenas do estado atual si e da ação ai, mas não de estados ou ações anteriores."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discounted Future Reward\r\n",
    "\r\n",
    "Para se comportar bem a longo prazo, precisamos levar em conta não só as recompensas imediatas, mas também as recompensas futuras que vamos conseguir. Como devemos fazer isso?\r\n",
    "\r\n",
    "Dada uma série do processo de decisão de Markov, podemos calcular facilmente a recompensa total de um episódio:\r\n",
    "\r\n",
    "<img src=\"images/reward1.png\">\r\n",
    "\r\n",
    "Dado que, a recompensa futura total a partir do ponto de tempo a seguir pode ser expressa como:\r\n",
    "\r\n",
    "<img src=\"images/reward2.png\">\r\n",
    "\r\n",
    "Mas como o nosso ambiente é estocástico, nunca podemos ter certeza, se conseguiremos as mesmas recompensas na próxima vez que realizarmos as mesmas ações. Quanto mais no futuro nós vamos, mais pode divergir. Por esse motivo, é comum usar uma recompensa futura com desconto em vez disso:\r\n",
    "\r\n",
    "<img src=\"images/reward3.png\">\r\n",
    "\r\n",
    "Aqui γ é o fator de desconto entre 0 e 1 - quanto mais no futuro a recompensa é, menos levamos em consideração. É fácil de ver, que a recompensa futura com desconto no tempo passo t pode ser expressa em termos da mesma coisa no tempo passo t + 1:\r\n",
    "\r\n",
    "<img src=\"images/reward4.png\">\r\n",
    "\r\n",
    "Se definimos o fator de desconto γ = 0, nossa estratégia será míope e confiamos apenas nas recompensas imediatas. Se quisermos equilibrar entre recompensas imediatas e futuras, devemos definir fator de desconto para algo como γ = 0.9. Se o nosso ambiente for determinista e as mesmas ações sempre resultarão nas mesmas recompensas, então podemos definir o fator de desconto γ = 1.\r\n",
    "\r\n",
    "Uma boa estratégia para um agente seria sempre escolher uma ação que maximize a recompensa futura (descontada)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Elementos do Aprendizado por Reforço\n",
    "\n",
    "Além do agente e do ambiente, pode-se identificar quatro subelementos principais de RL:\n",
    "\n",
    "* Política - é um mapeamento de estados percebidos do ambiente para ações a serem tomadas quando nesses estados. A política é o núcleo de um agente de aprendizado por reforço, no sentido de que por si só é suficiente para determinar o comportamento. Talvez seja estocástico, especificando probabilidades para cada ação.\n",
    "\n",
    "\n",
    "* Recompensas - Em cada etapa, o ambiente envia ao agente de aprendizado por reforço um único número chamado recompensa. O único objetivo do agente é maximizar a recompensa total que recebe a longo prazo. O sinal de recompensa define, portanto, quais são os sinais bons e ruins para o agente. Talvez seja uma função estocástica do estado e da ação.\n",
    "\n",
    "\n",
    "* Função de valor - especifica, a grosso modo, que o valor de um estado é a quantidade total de recompensa que um agente pode esperar acumular no futuro, a partir desse estado. Enquanto as recompensas determinam a conveniência imediata e intrínseca dos estados ambientais, os valores indicam a conveniência a longo prazo dos estados após levar em consideração os estados que provavelmente seguirão e as recompensas disponíveis nesses estados. Por exemplo, um estado sempre pode gerar uma recompensa imediata baixa, mas ainda tem um valor alto, porque é seguido regularmente por outros estados que geram recompensas altas ou o inverso pode ser verdadeiro.\n",
    "\n",
    "\n",
    "* Modelo do ambiente - imita o comportamento do ambiente, permitindo inferências sobre como o ambiente se comportará. Por exemplo, dado um estado e uma ação, o modelo pode prever o próximo estado resultante e a próxima recompensa. Os métodos para resolver problemas de aprendizado por reforço que usam modelos são chamados de métodos baseados em modelo, em oposição a métodos mais simples sem modelo, aprendizes de tentativa e erro.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As recompensas são, em certo sentido, primárias, enquanto os valores, como previsões de recompensas, são secundários. Sem recompensas, não poderia haver valores, e o único objetivo de estimar valores é obter mais recompensa. No entanto, são os valores que mais nos preocupam ao tomar e avaliar decisões."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Algoritmo de Aprendizado Por Reforço Para Resolver o Jogo da Velha"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abordagem de aprendizado por reforço para resolver o jogo da velha:\n",
    "\n",
    "\n",
    "* Configure a tabela de números, um para cada estado possível do jogo.\n",
    "\n",
    "* Cada número será nossa estimativa mais recente de nossa probabilidade de vencer a partir desse estado.\n",
    "\n",
    "* Essa estimativa é o valor do estado e a tabela inteira é a função de valor aprendido.\n",
    "\n",
    "* Supondo que sempre jogamos Xs, então para todos os estados com 3 Xs seguidos (coluna e diagonal) a probabilidade de ganhar é 1,0\n",
    "\n",
    "* E para todos os estados com 3 Os seguidos (coluna e diagonal), a probabilidade de vitória é de 0,0\n",
    "\n",
    "* Definimos os valores iniciais de todos os outros estados para 0,5 (representando uma chance de 50% de vitória)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Em seguida, jogamos muitos jogos contra o oponente. Para selecionar nossos movimentos:\n",
    "\n",
    "1. Examinamos os estados que resultariam de cada uma de nossas possíveis movimentações e procuramos seus valores atuais na tabela.\n",
    "\n",
    "2. Na maioria das vezes nos movemos com avidez, selecionando a jogada que leva ao estado com o maior valor. (maior probabilidade de ganhar).\n",
    "\n",
    "3. Ocasionalmente, selecionamos aleatoriamente dentre os outros movimentos. (Exploração)\n",
    "\n",
    "4. Enquanto jogamos, mudamos os valores dos estados em que nos encontramos.\n",
    "\n",
    "Depois de cada movimento \"ganancioso\", de A para B, atualizamos o valor de A para ficar mais próximo do valor de B.\n",
    "Isso é alcançado usando a seguinte fórmula:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.display import Image, display\r\n",
    "display(Image('images/qlearn.png'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementando Aprendizagem Por Reforço em Python"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos construir e treinar um agente para encontrar o caminho mais curto entre a posição `(0, 0)` até o canto oposto `(Ny-1, Nx-1)` de uma grade retangular 2D no ambiente 2D de tamanho (Ny, Nx). Ou seja, o agente deverá encontrar o melhor caminho do canto superior esquerdo da grade, até o canto inferior direito.\n",
    "\n",
    "O agente está restrito a se mover para subir / descer / esquerda / direita por 1 quadrado da grade por ação. O agente recebe uma penalidade de `-0,1` por cada ação que não atingir o estado terminal (para incentivar a busca por caminho mais curto) e uma recompensa de `100` ao atingir o estado terminal. O parâmetro de exploração do agente `epsilon` também decai por uma constante multiplicativa após cada episódio de treinamento. * As formas tabulares * do valor da ação * Q (s, a) *, recompensa * R (s, a) * e policy * P (s) * são usadas. Usaremos a Greed Policy (que busca o caminho mais curto entre dois pontos) para treinar o agente."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Imports\r\n",
    "import os, sys, random, operator\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Ambiente\r\n",
    "\r\n",
    "class Ambiente:\r\n",
    "    \r\n",
    "    def __init__(self, Ny=8, Nx=8):\r\n",
    "        \r\n",
    "        # Define o espaço de estado\r\n",
    "        self.Ny = Ny  \r\n",
    "        self.Nx = Nx  \r\n",
    "        self.state_dim = (Ny, Nx)\r\n",
    "        \r\n",
    "        # Definindo o espaço de ação\r\n",
    "        self.action_dim = (4,)  # para cima, para baixo, direita, esquerda\r\n",
    "        self.action_dict = {\"subir\": 0, \"direita\": 1, \"descer\": 2, \"esquerda\": 3}\r\n",
    "        self.action_coords = [(-1, 0), (0, 1), (1, 0), (0, -1)]  # transações (mudanças)\r\n",
    "        \r\n",
    "        # Definindo tabela de recompensas\r\n",
    "        self.R = self._build_rewards()  \r\n",
    "        \r\n",
    "        # Checando consistência do espaço de ação\r\n",
    "        if len(self.action_dict.keys()) != len(self.action_coords):\r\n",
    "            exit(\"Erro: Ação Inconsistente\")\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        # Redefinir o estado do agente para o canto superior esquerdo da grade\r\n",
    "        self.state = (0, 0)  \r\n",
    "        return self.state\r\n",
    "\r\n",
    "    def step(self, action):\r\n",
    "        # Evoluir o estado do agente\r\n",
    "        state_next = (self.state[0] + self.action_coords[action][0], self.state[1] + self.action_coords[action][1])\r\n",
    "        \r\n",
    "        # Coletar recompensa\r\n",
    "        reward = self.R[self.state + (action,)]\r\n",
    "        \r\n",
    "        # Terminar se chegarmos ao canto inferior direito da grade\r\n",
    "        done = (state_next[0] == self.Ny - 1) and (state_next[1] == self.Nx - 1)\r\n",
    "        \r\n",
    "        # Atualiza o estado\r\n",
    "        self.state = state_next\r\n",
    "        return state_next, reward, done\r\n",
    "    \r\n",
    "    def allowed_actions(self):\r\n",
    "        # Gerar lista de ações permitidas, dependendo da localização do agente na grade \r\n",
    "        actions_allowed = []\r\n",
    "        y, x = self.state[0], self.state[1]\r\n",
    "        \r\n",
    "        if (y > 0):  \r\n",
    "            actions_allowed.append(self.action_dict[\"subir\"])\r\n",
    "        if (y < self.Ny - 1):  \r\n",
    "            actions_allowed.append(self.action_dict[\"descer\"])\r\n",
    "        if (x > 0):  \r\n",
    "            actions_allowed.append(self.action_dict[\"esquerda\"])\r\n",
    "        if (x < self.Nx - 1): \r\n",
    "            actions_allowed.append(self.action_dict[\"direita\"])\r\n",
    "        \r\n",
    "        actions_allowed = np.array(actions_allowed, dtype=int)\r\n",
    "        return actions_allowed\r\n",
    "\r\n",
    "    def _build_rewards(self):\r\n",
    "        # Definir recompensas do agente R [s, a]\r\n",
    "        \r\n",
    "        # Recompensa por chegar ao estado terminal (canto inferior direito)\r\n",
    "        r_goal = 100  \r\n",
    "        \r\n",
    "        # Penalidade por não atingir o estado terminal\r\n",
    "        r_nongoal = -0.1  \r\n",
    "        \r\n",
    "        # R[s,a]\r\n",
    "        R = r_nongoal * np.ones(self.state_dim + self.action_dim, dtype=float)  \r\n",
    "        \r\n",
    "        # Chegar de cima\r\n",
    "        R[self.Ny - 2, self.Nx - 1, self.action_dict[\"descer\"]] = r_goal  \r\n",
    "        \r\n",
    "        # Chegar da esquerda\r\n",
    "        R[self.Ny - 1, self.Nx - 2, self.action_dict[\"direita\"]] = r_goal  \r\n",
    "        \r\n",
    "        return R\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "action_dict = {\"subir\": 0, \"direita\": 1, \"descer\": 2, \"esquerda\": 3}\r\n",
    "action_dim = (4,)\r\n",
    "state_dim = (8,8)\r\n",
    "\r\n",
    "print(state_dim + action_dim)\r\n",
    "r_ongol = -0.1\r\n",
    "\r\n",
    "R = r_ongol * np.ones(state_dim + action_dim, dtype=float)\r\n",
    "print(R.shape)\r\n",
    "print(R)\r\n",
    "\r\n",
    "\r\n",
    "R[6, 7, action_dict[\"descer\"]] = 100\r\n",
    "R[7, 6, action_dict[\"direita\"]] = 100\r\n",
    "\r\n",
    "print('-'.center(20, '*'))\r\n",
    "print(R)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(8, 8, 4)\n",
      "(8, 8, 4)\n",
      "[[[-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]]\n",
      "\n",
      " [[-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]]\n",
      "\n",
      " [[-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]]\n",
      "\n",
      " [[-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]]\n",
      "\n",
      " [[-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]]\n",
      "\n",
      " [[-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]]\n",
      "\n",
      " [[-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]]\n",
      "\n",
      " [[-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]\n",
      "  [-0.1 -0.1 -0.1 -0.1]]]\n",
      "*********-**********\n",
      "[[[ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]]\n",
      "\n",
      " [[ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]]\n",
      "\n",
      " [[ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]]\n",
      "\n",
      " [[ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]]\n",
      "\n",
      " [[ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]]\n",
      "\n",
      " [[ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]]\n",
      "\n",
      " [[ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1 100.   -0.1]]\n",
      "\n",
      " [[ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]\n",
      "  [ -0.1 100.   -0.1  -0.1]\n",
      "  [ -0.1  -0.1  -0.1  -0.1]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Agente\r\n",
    "class Agente:\r\n",
    "    \r\n",
    "    def __init__(self, env):\r\n",
    "        # Armazenar estado e dimensão de ação\r\n",
    "        self.state_dim = env.state_dim\r\n",
    "        self.action_dim = env.action_dim\r\n",
    "        \r\n",
    "        # Parâmetros de aprendizado do agente\r\n",
    "        \r\n",
    "        # Probabilidade inicial de exploração\r\n",
    "        self.epsilon = 1.0  \r\n",
    "        \r\n",
    "        # Epsilon decay após cada episódio\r\n",
    "        self.epsilon_decay = 0.99  \r\n",
    "        \r\n",
    "        # Learning rate\r\n",
    "        self.beta = 0.99 \r\n",
    "        \r\n",
    "         # Reward discount factor\r\n",
    "        self.gamma = 0.99 \r\n",
    "        \r\n",
    "        # Inicaliza a tabela Q[s,a] - Q Learning\r\n",
    "        self.Q = np.zeros(self.state_dim + self.action_dim, dtype=float)\r\n",
    "\r\n",
    "    def get_action(self, env):\r\n",
    "        # Política do Agente - Epsilon-greedy \r\n",
    "        if random.uniform(0, 1) < self.epsilon:\r\n",
    "            # Exploração\r\n",
    "            return np.random.choice(env.allowed_actions())\r\n",
    "        else:\r\n",
    "            # Explorar em ações permitidas\r\n",
    "            state = env.state;\r\n",
    "            actions_allowed = env.allowed_actions()\r\n",
    "            Q_s = self.Q[state[0], state[1], actions_allowed]\r\n",
    "            actions_greedy = actions_allowed[np.flatnonzero(Q_s == np.max(Q_s))]\r\n",
    "            return np.random.choice(actions_greedy)\r\n",
    "\r\n",
    "    def train(self, memory):\r\n",
    "        # -----------------------------\r\n",
    "        #\r\n",
    "        # Q[s,a] <- Q[s,a] + beta * (R[s,a] + gamma * max(Q[s,:]) - Q[s,a])\r\n",
    "        #\r\n",
    "        #  R[s,a] = recompensa por tomar uma ação do estado s\r\n",
    "        #  beta = learning rate\r\n",
    "        #  gamma = discount factor\r\n",
    "        # -----------------------------\r\n",
    "        (state, action, state_next, reward, done) = memory\r\n",
    "        sa = state + (action,)\r\n",
    "        self.Q[sa] += self.beta * (reward + self.gamma*np.max(self.Q[state_next]) - self.Q[sa])\r\n",
    "\r\n",
    "    def display_greedy_policy(self):\r\n",
    "        # Greedy policy = argmax[a'] Q[s,a']\r\n",
    "        greedy_policy = np.zeros((self.state_dim[0], self.state_dim[1]), dtype=int)\r\n",
    "        for x in range(self.state_dim[0]):\r\n",
    "            for y in range(self.state_dim[1]):\r\n",
    "                greedy_policy[y, x] = np.argmax(self.Q[y, x, :])\r\n",
    "        print(\"\\nGrade(y, x):\")\r\n",
    "        print(greedy_policy)\r\n",
    "        print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def main():\r\n",
    "\r\n",
    "    # Configurações\r\n",
    "    env = Ambiente(Ny=8, Nx=8) \r\n",
    "    agent = Agente(env)\r\n",
    "\r\n",
    "    # Treinamento do agente\r\n",
    "    print(\"\\nTreinando o Agente...\\n\")\r\n",
    "    \r\n",
    "    N_episodes = 500\r\n",
    "    \r\n",
    "    for episode in range(N_episodes):\r\n",
    "\r\n",
    "        # Gera um episódio\r\n",
    "        iter_episode, reward_episode = 0, 0  \r\n",
    "        \r\n",
    "        # Estado inicial\r\n",
    "        state = env.reset()  \r\n",
    "        \r\n",
    "        while True:\r\n",
    "            \r\n",
    "            # Obtém uma ação\r\n",
    "            action = agent.get_action(env)  \r\n",
    "            \r\n",
    "            # Evoluir estado por ação\r\n",
    "            state_next, reward, done = env.step(action) \r\n",
    "            \r\n",
    "            # Treina o agente\r\n",
    "            agent.train((state, action, state_next, reward, done))  \r\n",
    "            iter_episode += 1\r\n",
    "            reward_episode += reward \r\n",
    "            \r\n",
    "            if done:\r\n",
    "                break\r\n",
    "            \r\n",
    "            # Transição para o próximo estado\r\n",
    "            state = state_next  \r\n",
    "\r\n",
    "        # Parâmetro de exploração do agente \r\n",
    "        agent.epsilon = max(agent.epsilon * agent.epsilon_decay, 0.01)\r\n",
    "\r\n",
    "        # Print\r\n",
    "        if (episode == 0) or (episode + 1) % 10 == 0: \r\n",
    "            print(\"[Episódio {}/{}] Parâmetro de Exploração = {:.3F} -> Iteração = {}, Recomenpensa = {:.1F}\".format(\r\n",
    "                episode + 1, N_episodes, agent.epsilon, iter_episode, reward_episode))\r\n",
    "\r\n",
    "        # Print greedy policy\r\n",
    "        if (episode == N_episodes - 1):\r\n",
    "            agent.display_greedy_policy()\r\n",
    "            for (key, val) in sorted(env.action_dict.items(), key=operator.itemgetter(1)):\r\n",
    "                print(\"Ação['{}'] = {}\".format(key, val))\r\n",
    "            print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Execução\r\n",
    "if __name__ == '__main__':\r\n",
    "    main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Treinando o Agente...\n",
      "\n",
      "[Episódio 1/500] Parâmetro de Exploração = 0.990 -> Iteração = 126, Recomenpensa = 87.5\n",
      "[Episódio 10/500] Parâmetro de Exploração = 0.904 -> Iteração = 110, Recomenpensa = 89.1\n",
      "[Episódio 20/500] Parâmetro de Exploração = 0.818 -> Iteração = 28, Recomenpensa = 97.3\n",
      "[Episódio 30/500] Parâmetro de Exploração = 0.740 -> Iteração = 92, Recomenpensa = 90.9\n",
      "[Episódio 40/500] Parâmetro de Exploração = 0.669 -> Iteração = 44, Recomenpensa = 95.7\n",
      "[Episódio 50/500] Parâmetro de Exploração = 0.605 -> Iteração = 42, Recomenpensa = 95.9\n",
      "[Episódio 60/500] Parâmetro de Exploração = 0.547 -> Iteração = 26, Recomenpensa = 97.5\n",
      "[Episódio 70/500] Parâmetro de Exploração = 0.495 -> Iteração = 58, Recomenpensa = 94.3\n",
      "[Episódio 80/500] Parâmetro de Exploração = 0.448 -> Iteração = 38, Recomenpensa = 96.3\n",
      "[Episódio 90/500] Parâmetro de Exploração = 0.405 -> Iteração = 20, Recomenpensa = 98.1\n",
      "[Episódio 100/500] Parâmetro de Exploração = 0.366 -> Iteração = 16, Recomenpensa = 98.5\n",
      "[Episódio 110/500] Parâmetro de Exploração = 0.331 -> Iteração = 18, Recomenpensa = 98.3\n",
      "[Episódio 120/500] Parâmetro de Exploração = 0.299 -> Iteração = 26, Recomenpensa = 97.5\n",
      "[Episódio 130/500] Parâmetro de Exploração = 0.271 -> Iteração = 16, Recomenpensa = 98.5\n",
      "[Episódio 140/500] Parâmetro de Exploração = 0.245 -> Iteração = 20, Recomenpensa = 98.1\n",
      "[Episódio 150/500] Parâmetro de Exploração = 0.221 -> Iteração = 18, Recomenpensa = 98.3\n",
      "[Episódio 160/500] Parâmetro de Exploração = 0.200 -> Iteração = 18, Recomenpensa = 98.3\n",
      "[Episódio 170/500] Parâmetro de Exploração = 0.181 -> Iteração = 20, Recomenpensa = 98.1\n",
      "[Episódio 180/500] Parâmetro de Exploração = 0.164 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 190/500] Parâmetro de Exploração = 0.148 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 200/500] Parâmetro de Exploração = 0.134 -> Iteração = 16, Recomenpensa = 98.5\n",
      "[Episódio 210/500] Parâmetro de Exploração = 0.121 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 220/500] Parâmetro de Exploração = 0.110 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 230/500] Parâmetro de Exploração = 0.099 -> Iteração = 18, Recomenpensa = 98.3\n",
      "[Episódio 240/500] Parâmetro de Exploração = 0.090 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 250/500] Parâmetro de Exploração = 0.081 -> Iteração = 16, Recomenpensa = 98.5\n",
      "[Episódio 260/500] Parâmetro de Exploração = 0.073 -> Iteração = 16, Recomenpensa = 98.5\n",
      "[Episódio 270/500] Parâmetro de Exploração = 0.066 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 280/500] Parâmetro de Exploração = 0.060 -> Iteração = 16, Recomenpensa = 98.5\n",
      "[Episódio 290/500] Parâmetro de Exploração = 0.054 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 300/500] Parâmetro de Exploração = 0.049 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 310/500] Parâmetro de Exploração = 0.044 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 320/500] Parâmetro de Exploração = 0.040 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 330/500] Parâmetro de Exploração = 0.036 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 340/500] Parâmetro de Exploração = 0.033 -> Iteração = 16, Recomenpensa = 98.5\n",
      "[Episódio 350/500] Parâmetro de Exploração = 0.030 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 360/500] Parâmetro de Exploração = 0.027 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 370/500] Parâmetro de Exploração = 0.024 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 380/500] Parâmetro de Exploração = 0.022 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 390/500] Parâmetro de Exploração = 0.020 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 400/500] Parâmetro de Exploração = 0.018 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 410/500] Parâmetro de Exploração = 0.016 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 420/500] Parâmetro de Exploração = 0.015 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 430/500] Parâmetro de Exploração = 0.013 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 440/500] Parâmetro de Exploração = 0.012 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 450/500] Parâmetro de Exploração = 0.011 -> Iteração = 16, Recomenpensa = 98.5\n",
      "[Episódio 460/500] Parâmetro de Exploração = 0.010 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 470/500] Parâmetro de Exploração = 0.010 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 480/500] Parâmetro de Exploração = 0.010 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 490/500] Parâmetro de Exploração = 0.010 -> Iteração = 14, Recomenpensa = 98.7\n",
      "[Episódio 500/500] Parâmetro de Exploração = 0.010 -> Iteração = 14, Recomenpensa = 98.7\n",
      "\n",
      "Grade(y, x):\n",
      "[[1 1 1 1 2 2 2 3]\n",
      " [1 1 1 1 1 2 2 2]\n",
      " [1 1 1 1 1 2 2 2]\n",
      " [1 1 1 1 1 1 2 2]\n",
      " [1 1 1 1 1 1 1 2]\n",
      " [0 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 0]]\n",
      "\n",
      "Ação['subir'] = 0\n",
      "Ação['direita'] = 1\n",
      "Ação['descer'] = 2\n",
      "Ação['esquerda'] = 3\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fim"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "13cdb1a53a3b8f4408fb1847c39a4e43de27b2e104c0ea385fb2ef9b47416461"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}